# List of researchers/resources to review:

## Statistical Machine Learning, DNN Theory, Convergence, and Optimization
1. Arora, Sanjeev, et al. "Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks." http://arxiv.org/abs/1901.08584 (2019).
2. Lee et. al. / Deep Neural Networks as Gaussian Processes / https://arxiv.org/abs/1711.00165v3
3. Arora, Sanjeev, Nadav Cohen, and Elad Hazan. "On the optimization of deep networks: Implicit acceleration by overparameterization." http://arxiv.org/abs/1802.06509 (2018).
4. Nagarajan, Vaishnavh, and J. Zico Kolter. "Uniform convergence may be unable to explain generalization in deep learning." Advances in Neural Information Processing Systems. 2019.
5. Bartlett, Peter L., et al. "Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks." Journal of Machine Learning Research 20.63 (2019): 1-17.
6. Du, Simon S., et al. "Gradient descent finds global minima of deep neural networks." http://arxiv.org/abs/1811.03804 (2018).
7. Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., andSrebro, N. The role of over-parametrization in generalization of neural networks. In International Conference on Learning Representations (ICLR), 2019.
8. Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking generalization. International Conference on Learning Represen
tations (ICLR), 2017
9. Brutzkus, Alon, et al. "Sgd learns over-parameterized networks that provably generalize on linearly separable data." http://arxiv.org/abs/1710.10174 (2017).
10. Arora, Sanjeev, et al. "Stronger generalization bounds for deep nets via a compression approach." http://arxiv.org/abs/1802.05296 (2018).
11. Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017.
12. Belkin, M., Ma, S., and Mandal, S. To understand deep learning we need to understand kernel learning. http://arxiv.org/abs/1802.01396, 2018.
13. Daniely, A. SGD learns the conjugate kernel class of the network. http://arxiv.org/abs/1702.08503, 2017
14. Jacot, Arthur, Franck Gabriel, and Clément Hongler. "Neural tangent kernel: Convergence and generalization in neural networks." Advances in neural information processing systems. 2018.
15. Amit, Ron, and Ron Meir. "Meta-learning by adjusting priors based on extended PAC-Bayes theory." https://arxiv.org/abs/1711.01244
16. Tishby, Naftali, and Noga Zaslavsky. "Deep learning and the information bottleneck principle." 2015 IEEE Information Theory Workshop (ITW). IEEE, 2015.
17. Fortunin et. al. / GP-VAE: Deep Probabilistic Time Series Imputation / https://arxiv.org/abs/1907.04155v5
18. Cutajar et. al. / Deep Gaussian Processes for Multi-fidelity Modeling / https://arxiv.org/abs/1903.07320v1




## Supervised Learning

## Unsupervised Learning
1. Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." http://arxiv.org/abs/1312.6114 (2013).
2.Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. International Conference on Machine Learning, pp. 1278–1286, 2014.
3. Burda, Yuri, Roger Grosse, and Ruslan Salakhutdinov. "Importance weighted autoencoders." http://arxiv.org/abs/1509.00519 (2015).
4. Goodfellow, Ian, et al. "Generative adversarial nets." Advances in neural information processing systems. 2014.
5. Grnarova, Paulina, et al. "A domain agnostic measure for monitoring and evaluating GANs." Advances in Neural Information Processing Systems. 2019.
6. Dziugaite, G. K., Roy, D. M., and Ghahramani, Z. Training generative neural networks via maximum mean discrepancy optimization, 2015. http://arxiv.org/abs/1505.0390
7. Li, Y., Swersky, K., and Zemel, R. Generative moment matching networks. In Proceedings of the 32nd International Conference on Machine Learning, 2015
8. Bengio, Y., Thibodeau-Laufer, E., and Yosinski, J. (2014a). Deep generative stochastic networks trainable by backprop. In ICML’14.
9. Arora, Sanjeev, et al. "Generalization and equilibrium in generative adversarial nets (gans)." Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.10. Nowozin, Sebastian, Botond Cseke, and Ryota Tomioka. "f-gan: Training generative neural samplers using variational divergence minimization." Advances in neural information processing systems. 2016.
11. Arjovsky, Martin, Soumith Chintala, and Léon Bottou. "Wasserstein gan." http://arxiv.org/abs/1701.07875 (2017).
12. Gulrajani, Ishaan, et al. "Improved training of wasserstein gans." Advances in neural information processing systems. 2017.

## Semi-supervised Learning

## Meta-Learning
1. Park, Sangwoo, Osvaldo Simeone, and Joonhyuk Kang. "End-to-End Fast Training of Communication Links Without a Channel Model via Online Meta-Learning." http://arxiv.org/abs/2003.01479 (2020).
2. Simeone, Osvaldo, Sangwoo Park, and Joonhyuk Kang. "From Learning to Meta-Learning: Reduced Training Overhead and Complexity for Communication Systems." http://arxiv.org/abs/2001.01227 (2020).

## Multiagent and Federated Learning
1. Konečný, Jakub, et al. "Federated learning: Strategies for improving communication efficiency." http://arxiv.org/abs/1610.05492 (2016).
2. Xing, Hong, Osvaldo Simeone, and Suzhi Bi. "Decentralized Federated Learning via SGD over Wireless D2D Networks." http://arxiv.org/abs/2002.12507 (2020).
3. Ahn, Jin-Hyun, Osvaldo Simeone, and Joonhyuk Kang. "Cooperative Learning via Federated Distillation over Fading Channels." http://arxiv.org/abs/2002.01337 (2020).
4. Zhao, Yue, et al. "Federated learning with non-iid data." arXiv preprint arXiv:1806.00582 (2018).https://arxiv.org/abs/1806.00582
5. Nguyen, Huy, et al. "Resource Allocation in Mobility-Aware Federated Learning Networks: A Deep Reinforcement Learning Approach." https://arxiv.org/abs/1910.09172 (2019).
6. Kairouz, Peter, et al. "Advances and Open Problems in Federated Learning." https://arxiv.org/abs/1912.04977 (2019).

## ML in Communication Systems
1. Naderializadeh, Navid, et al. "Resource Management in Wireless Networks via Multi-Agent Deep Reinforcement Learning." http://arxiv.org/abs/2002.06215 (2020).
2. Naparstek, Oshri, and Kobi Cohen. "Deep multi-user reinforcement learning for distributed dynamic spectrum access." IEEE Transactions on Wireless Communications 18.1 (2018): 310-323.
3. Nisioti, Eleni, and Nikolaos Thomos. "Fast Q-learning for Improved Finite Length Performance of Irregular Repetition Slotted ALOHA." IEEE Transactions on Cognitive Communications and Networking (2019).

